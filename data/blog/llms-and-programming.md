---
title: LLMs and programming
summary: Making the most of an emerging technology
tags: [testing, serverless, aws]
draft: true
---

It was very easy to get caught up in the whirlwind of hype around LLMs back when ChatGPT first hit the tech blogs _when_ months ago. The rush of excitement at the potential of these new assistants along with a crypto-esque scramble to try and build / montize stuff and ride the wave, so to speak. _time_ later, the fizz has defintiely settled and whilst I feel like LLMs are _stil_ an incredible tool when used properly, they are not without shortcomings and should be used like any tool, with consideration.

I use an LLM pretty much every day, for my work, for writing (although not for this post, promise!) and for general technical queries.
It has to be said, that for very specific tasks I feel like LLMs are almost second to none as a source for advice. They are infinitely patient, accurrate when correctly prompted and a lot of the time, correct.

A few use cases I've found:

- _talk me through doing x task_ for example; debugging a post-upgrade NextJS build & deploy issue.
- _rewrite this code to acheive x_ given example code, using a similar style or with the same outcome, improve the performance, use library y, improve readability, modularize, etc.
- _given huge and undigestable stack trace or log output, point me in the right direction_
- \_given general project structure, suggest some architectural options to achieve x
